{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fe8bbb",
   "metadata": {},
   "source": [
    "# MinHash-basierte Textähnlichkeitsanalyse\n",
    "\n",
    "Dieses Notebook implementiert MinHash zur effizienten Berechnung der Jaccard-Ähnlichkeit zwischen Textdokumenten.\n",
    "\n",
    "## Konzept\n",
    "- **MinHash**: Probabilistische Datenstruktur zur Schätzung der Jaccard-Ähnlichkeit\n",
    "- **Vorteil**: Skaliert sehr gut für große Dokumentsammlungen\n",
    "- **Anwendung**: Direkte Textanalyse der OPAL-Materialien\n",
    "- **Output**: Ähnlichkeitsmatrix basierend auf Textinhalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb38282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import aller benötigten Bibliotheken für MinHash\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import gc\n",
    "from datasketch import MinHash\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7132ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from DataHandler import DataHandler\n",
    "\n",
    "dataHandler = DataHandler(\"../config.yaml\")\n",
    "config_manager = dataHandler.config_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afbde4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Lade: OPAL_ai_meta.p\n",
      "   ✅ Geladen: 4,548 Zeilen × 25 Spalten\n",
      "📊 Geladen: 4548 OPAL-Materialien\n",
      "📁 Content-Ordner: /media/sz/Data/Connected_Lecturers/Opal/raw/content\n"
     ]
    }
   ],
   "source": [
    "# Lade OPAL-Metadaten und konfiguriere Content-Ordner\n",
    "df_aimeta = dataHandler.load_data(\"data_files.raw_data.df_aimeta\")\n",
    "content_folder = dataHandler.config_manager.get('folder_structure.raw_content')\n",
    "\n",
    "print(f\"📊 Geladen: {len(df_aimeta)} OPAL-Materialien\")\n",
    "print(f\"📁 Content-Ordner: {content_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24982d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 SCHRITT 1: DATEN VORBEREITEN\n",
      "----------------------------------------\n",
      "🔢 Anzahl zu verarbeitender Materialien: 4548\n",
      "📋 pipe:ID Bereich: 1-FnRgnGtuu4 bis 9zrgYh873fJ4\n",
      "\n",
      "⚙️ KONFIGURATION\n",
      "----------------------------------------\n",
      "📄 Text-Features Konfiguration:\n",
      "   📖 Content einbeziehen: ✅\n",
      "   📝 Titel einbeziehen: ❌ (Gewichtung: 3x)\n",
      "   🏷️  Keywords einbeziehen: ❌ (Gewichtung: 2x)\n",
      "   ⚙️ Shingle-Konfiguration: 3-gram Shingles, 128 Permutationen\n",
      "\n",
      "🔤 SCHRITT 2: TEXT-FEATURES EXTRAHIEREN\n",
      "----------------------------------------\n",
      "\n",
      "📄 Lade Content-Dateien und kombiniere mit Metadaten...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeite Materialien:   0%|          | 0/4548 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeite Materialien: 100%|██████████| 4548/4548 [00:42<00:00, 106.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Erfolgreich verarbeitete Materialien: 4548\n",
      "❌ Fehlgeschlagene Content-Loads: 0\n",
      "📊 Content-Verfügbarkeitsrate: 100.0%\n",
      "📊 Gesamt-Verarbeitungsrate: 100.0%\n",
      "\n",
      "📝 Verwendete Text-Features:\n",
      "   1. Content (Haupttext)\n",
      "📈 Shingle-Statistiken:\n",
      "   Durchschnitt: 1771.2\n",
      "   Median: 714.5\n",
      "   Min: 22\n",
      "   Max: 83437\n",
      "\n",
      "💾 Verwende 4548 Materialien für MinHash-Berechnung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Bereinigt Text für MinHash-Verarbeitung\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Konvertiere zu String falls noch nicht\n",
    "    text = str(text)\n",
    "    \n",
    "    # Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Entferne Interpunktion und Sonderzeichen\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Entferne mehrfache Leerzeichen\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Entferne führende/nachfolgende Leerzeichen\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_shingles(text, k=3):\n",
    "    \"\"\"Erstellt k-Shingles aus Text\"\"\"\n",
    "    if not text or len(text) < k:\n",
    "        return set()\n",
    "    \n",
    "    words = text.split()\n",
    "    if len(words) < k:\n",
    "        return {' '.join(words)}\n",
    "    \n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = ' '.join(words[i:i+k])\n",
    "        shingles.add(shingle)\n",
    "    \n",
    "    return shingles\n",
    "\n",
    "def load_content_file(pipe_id, content_folder):\n",
    "    \"\"\"Lädt den Inhalt einer Content-Datei basierend auf pipe:ID\"\"\"\n",
    "    content_path = Path(content_folder)\n",
    "    \n",
    "    # Suche nach Datei mit pipe_id im Namen\n",
    "    # Format ist meist: {pipe_id}.txt oder ähnlich\n",
    "    possible_files = list(content_path.glob(f\"*{pipe_id}*\"))\n",
    "    \n",
    "    if not possible_files:\n",
    "        return \"\"\n",
    "    \n",
    "    # Nimm die erste gefundene Datei\n",
    "    file_path = possible_files[0]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Fehler beim Laden von {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"\\n📋 SCHRITT 1: DATEN VORBEREITEN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verwende alle verfügbaren Materialien\n",
    "valid_materials = df_aimeta.copy()\n",
    "print(f\"🔢 Anzahl zu verarbeitender Materialien: {len(valid_materials)}\")\n",
    "\n",
    "# Erstelle Mapping von Index zu pipe:ID\n",
    "pipe_ids = valid_materials['pipe:ID'].tolist()\n",
    "print(f\"📋 pipe:ID Bereich: {min(pipe_ids)} bis {max(pipe_ids)}\")\n",
    "\n",
    "print(\"\\n⚙️ KONFIGURATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Konfiguration für Text-Features (anpassbar)\n",
    "INCLUDE_CONTENT = True      # Haupttext-Content einbeziehen\n",
    "INCLUDE_TITLE = False        # Titel einbeziehen\n",
    "INCLUDE_KEYWORDS = False     # Keywords einbeziehen\n",
    "\n",
    "# Gewichtung der verschiedenen Textquellen\n",
    "TITLE_WEIGHT = 3           # Titel 3x gewichten (da sehr relevant)\n",
    "KEYWORDS_WEIGHT = 2        # Keywords 2x gewichten (wichtige Schlagwörter)\n",
    "\n",
    "# Konfiguration für Shingles\n",
    "k_shingles = 3  # 3-gram Shingles\n",
    "num_perm = 128  # Anzahl Permutationen für MinHash\n",
    "\n",
    "print(f\"📄 Text-Features Konfiguration:\")\n",
    "print(f\"   📖 Content einbeziehen: {'✅' if INCLUDE_CONTENT else '❌'}\")\n",
    "print(f\"   📝 Titel einbeziehen: {'✅' if INCLUDE_TITLE else '❌'} (Gewichtung: {TITLE_WEIGHT}x)\")\n",
    "print(f\"   🏷️  Keywords einbeziehen: {'✅' if INCLUDE_KEYWORDS else '❌'} (Gewichtung: {KEYWORDS_WEIGHT}x)\")\n",
    "print(f\"   ⚙️ Shingle-Konfiguration: {k_shingles}-gram Shingles, {num_perm} Permutationen\")\n",
    "\n",
    "print(\"\\n🔤 SCHRITT 2: TEXT-FEATURES EXTRAHIEREN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sammle alle Text-Features\n",
    "all_documents = {}\n",
    "failed_loads = 0\n",
    "\n",
    "print(\"\\n📄 Lade Content-Dateien und kombiniere mit Metadaten...\")\n",
    "\n",
    "for idx, (df_idx, row) in enumerate(tqdm(valid_materials.iterrows(), total=len(valid_materials), desc=\"Verarbeite Materialien\")):\n",
    "    pipe_id = row['pipe:ID']\n",
    "    \n",
    "    # 1. Lade Content-Datei (optional)\n",
    "    content_text = \"\"\n",
    "    if INCLUDE_CONTENT:\n",
    "        content_text = load_content_file(pipe_id, content_folder)\n",
    "        if not content_text.strip():\n",
    "            failed_loads += 1\n",
    "    \n",
    "    # 2. Extrahiere Titel (optional)\n",
    "    title = \"\"\n",
    "    if INCLUDE_TITLE:\n",
    "        title = row.get('ai:title', '')\n",
    "        if pd.isna(title):\n",
    "            title = ''\n",
    "    \n",
    "    # 3. Extrahiere Keywords (optional) - verbesserte Behandlung für Arrays\n",
    "    keywords_text = \"\"\n",
    "    if INCLUDE_KEYWORDS:\n",
    "        keywords = row.get('valid_ddc_keywords', [])\n",
    "        \n",
    "        try:\n",
    "            if keywords is not None and hasattr(keywords, '__iter__') and not isinstance(keywords, str):\n",
    "                # Es ist eine Liste/Array\n",
    "                keywords_list = [str(kw).strip() for kw in keywords if kw is not None and str(kw).strip()]\n",
    "                keywords_text = ' '.join(keywords_list)\n",
    "            elif keywords is not None:\n",
    "                # Es ist ein einzelner Wert\n",
    "                keywords_text = str(keywords).strip()\n",
    "        except Exception as e:\n",
    "            # Falls etwas schiefgeht, ignoriere Keywords für dieses Dokument\n",
    "            keywords_text = \"\"\n",
    "    \n",
    "    # 4. Kombiniere alle aktivierten Textquellen\n",
    "    combined_text_parts = []\n",
    "    \n",
    "    if INCLUDE_CONTENT and content_text.strip():\n",
    "        combined_text_parts.append(content_text)\n",
    "    \n",
    "    if INCLUDE_TITLE and title.strip():\n",
    "        # Titel entsprechend der Gewichtung hinzufügen\n",
    "        combined_text_parts.extend([title] * TITLE_WEIGHT)\n",
    "    \n",
    "    if INCLUDE_KEYWORDS and keywords_text.strip():\n",
    "        # Keywords entsprechend der Gewichtung hinzufügen\n",
    "        combined_text_parts.extend([keywords_text] * KEYWORDS_WEIGHT)\n",
    "    \n",
    "    # Kombiniere zu einem Dokument\n",
    "    if combined_text_parts:\n",
    "        combined_text = ' '.join(combined_text_parts)\n",
    "        cleaned_text = clean_text(combined_text)\n",
    "        \n",
    "        # Erstelle Shingles\n",
    "        shingles = create_shingles(cleaned_text, k_shingles)\n",
    "        \n",
    "        if shingles:  # Nur wenn Shingles vorhanden\n",
    "            all_documents[pipe_id] = shingles\n",
    "    \n",
    "    # Memory cleanup alle 500 Iterationen\n",
    "    if idx % 500 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\n✅ Erfolgreich verarbeitete Materialien: {len(all_documents)}\")\n",
    "if INCLUDE_CONTENT:\n",
    "    print(f\"❌ Fehlgeschlagene Content-Loads: {failed_loads}\")\n",
    "    print(f\"📊 Content-Verfügbarkeitsrate: {(len(valid_materials)-failed_loads)/len(valid_materials)*100:.1f}%\")\n",
    "print(f\"📊 Gesamt-Verarbeitungsrate: {len(all_documents)/len(valid_materials)*100:.1f}%\")\n",
    "\n",
    "# Zeige verwendete Text-Features\n",
    "print(f\"\\n📝 Verwendete Text-Features:\")\n",
    "active_features = []\n",
    "if INCLUDE_CONTENT:\n",
    "    active_features.append(f\"Content (Haupttext)\")\n",
    "if INCLUDE_TITLE:\n",
    "    active_features.append(f\"Titel (Gewichtung: {TITLE_WEIGHT}x)\")\n",
    "if INCLUDE_KEYWORDS:\n",
    "    active_features.append(f\"Keywords (Gewichtung: {KEYWORDS_WEIGHT}x)\")\n",
    "\n",
    "for i, feature in enumerate(active_features, 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "\n",
    "if not active_features:\n",
    "    print(\"   ⚠️ WARNUNG: Keine Text-Features aktiviert!\")\n",
    "\n",
    "# Zeige Statistiken\n",
    "if all_documents:\n",
    "    shingle_counts = [len(shingles) for shingles in all_documents.values()]\n",
    "    print(f\"📈 Shingle-Statistiken:\")\n",
    "    print(f\"   Durchschnitt: {np.mean(shingle_counts):.1f}\")\n",
    "    print(f\"   Median: {np.median(shingle_counts):.1f}\")\n",
    "    print(f\"   Min: {min(shingle_counts)}\")\n",
    "    print(f\"   Max: {max(shingle_counts)}\")\n",
    "\n",
    "print(f\"\\n💾 Verwende {len(all_documents)} Materialien für MinHash-Berechnung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd84039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧮 SCHRITT 3: MINHASH-BERECHNUNG\n",
      "----------------------------------------\n",
      "🔢 Erstelle MinHash-Signaturen für 4548 Dokumente...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash Signaturen: 100%|██████████| 4548/4548 [01:12<00:00, 62.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MinHash-Signaturen erstellt für 4548 Dokumente\n",
      "\n",
      "📐 SCHRITT 4: ÄHNLICHKEITSMATRIX BERECHNEN\n",
      "----------------------------------------\n",
      "🔢 Berechne 4548 × 4548 Ähnlichkeitsmatrix...\n",
      "💭 Erwartete Berechnungen: 10,339,878 Paare\n",
      "🔄 Berechne paarweise Ähnlichkeiten...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ähnlichkeiten: 100%|██████████| 10339878/10339878 [00:50<00:00, 206443.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ähnlichkeitsmatrix berechnet: (4548, 4548)\n",
      "\n",
      "📊 ÄHNLICHKEITSMATRIX-STATISTIKEN:\n",
      "   📏 Matrix-Größe: 4548 × 4548\n",
      "   🔢 Gesamte Einträge: 20,684,304\n",
      "   📈 Ähnlichkeits-Statistiken (ohne Diagonale):\n",
      "      Durchschnitt: 0.0005\n",
      "      Median: 0.0000\n",
      "      Std-Abweichung: 0.0097\n",
      "      Min: 0.0000\n",
      "      Max: 1.0000\n",
      "\n",
      "💾 Bereite finale DataFrames vor...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🧮 SCHRITT 3: MINHASH-BERECHNUNG\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Erstelle MinHash-Signaturen für alle Dokumente\n",
    "document_minhashes = {}\n",
    "document_ids = list(all_documents.keys())\n",
    "\n",
    "print(f\"🔢 Erstelle MinHash-Signaturen für {len(document_ids)} Dokumente...\")\n",
    "\n",
    "for idx, doc_id in enumerate(tqdm(document_ids, desc=\"MinHash Signaturen\")):\n",
    "    # Erstelle MinHash für dieses Dokument\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    \n",
    "    # Füge alle Shingles zu MinHash hinzu\n",
    "    for shingle in all_documents[doc_id]:\n",
    "        m.update(shingle.encode('utf8'))\n",
    "    \n",
    "    document_minhashes[doc_id] = m\n",
    "    \n",
    "    # Memory cleanup alle 1000 Dokumente\n",
    "    if idx % 1000 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"✅ MinHash-Signaturen erstellt für {len(document_minhashes)} Dokumente\")\n",
    "\n",
    "print(\"\\n📐 SCHRITT 4: ÄHNLICHKEITSMATRIX BERECHNEN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Erstelle Ähnlichkeitsmatrix\n",
    "print(f\"🔢 Berechne {len(document_ids)} × {len(document_ids)} Ähnlichkeitsmatrix...\")\n",
    "print(f\"💭 Erwartete Berechnungen: {len(document_ids) * (len(document_ids) - 1) // 2:,} Paare\")\n",
    "\n",
    "# Initialisiere Matrix\n",
    "similarity_matrix = np.zeros((len(document_ids), len(document_ids)))\n",
    "\n",
    "# Berechne Ähnlichkeiten\n",
    "total_comparisons = len(document_ids) * (len(document_ids) - 1) // 2\n",
    "processed_comparisons = 0\n",
    "\n",
    "print(\"🔄 Berechne paarweise Ähnlichkeiten...\")\n",
    "\n",
    "with tqdm(total=total_comparisons, desc=\"Ähnlichkeiten\") as pbar:\n",
    "    for i in range(len(document_ids)):\n",
    "        doc_id_1 = document_ids[i]\n",
    "        minhash_1 = document_minhashes[doc_id_1]\n",
    "        \n",
    "        # Setze Diagonale auf 1.0 (Dokument ist mit sich selbst identisch)\n",
    "        similarity_matrix[i, i] = 1.0\n",
    "        \n",
    "        for j in range(i + 1, len(document_ids)):\n",
    "            doc_id_2 = document_ids[j]\n",
    "            minhash_2 = document_minhashes[doc_id_2]\n",
    "            \n",
    "            # Berechne Jaccard-Ähnlichkeit\n",
    "            jaccard_sim = minhash_1.jaccard(minhash_2)\n",
    "            \n",
    "            # Symmetrische Matrix\n",
    "            similarity_matrix[i, j] = jaccard_sim\n",
    "            similarity_matrix[j, i] = jaccard_sim\n",
    "            \n",
    "            processed_comparisons += 1\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Memory cleanup alle 100 Zeilen\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "print(f\"✅ Ähnlichkeitsmatrix berechnet: {similarity_matrix.shape}\")\n",
    "\n",
    "# Statistiken der Ähnlichkeitsmatrix\n",
    "non_diagonal_values = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "print(f\"\\n📊 ÄHNLICHKEITSMATRIX-STATISTIKEN:\")\n",
    "print(f\"   📏 Matrix-Größe: {similarity_matrix.shape[0]} × {similarity_matrix.shape[1]}\")\n",
    "print(f\"   🔢 Gesamte Einträge: {similarity_matrix.size:,}\")\n",
    "print(f\"   📈 Ähnlichkeits-Statistiken (ohne Diagonale):\")\n",
    "print(f\"      Durchschnitt: {np.mean(non_diagonal_values):.4f}\")\n",
    "print(f\"      Median: {np.median(non_diagonal_values):.4f}\")\n",
    "print(f\"      Std-Abweichung: {np.std(non_diagonal_values):.4f}\")\n",
    "print(f\"      Min: {np.min(non_diagonal_values):.4f}\")\n",
    "print(f\"      Max: {np.max(non_diagonal_values):.4f}\")\n",
    "\n",
    "print(f\"\\n💾 Bereite finale DataFrames vor...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30316dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 SCHRITT 5: FINALE DATAFRAMES ERSTELLEN UND SPEICHERN\n",
      "------------------------------------------------------------\n",
      "📊 Erstelle Ähnlichkeits-DataFrame mit pipe:IDs...\n",
      "✅ DataFrame erstellt:\n",
      "   📊 Ähnlichkeitsmatrix: (4548, 4548)\n",
      "\n",
      "💾 Speichere Matrix...\n",
      "💾 Datei gespeichert: minhash_text_similarity.p\n",
      "   📁 Pfad: /media/sz/Data/Connected_Lecturers/Opal/processed/similarity/minhash_text_similarity.p\n",
      "   📊 DataFrame: 4,548 Zeilen × 4548 Spalten\n",
      "   📏 Dateigröße: 157.9 MB\n",
      "   🕐 Zeitstempel: 2025-07-29 15:04:08\n",
      "   ⏱️  Speicherdauer: 0.06 Sekunden\n",
      "✅ Ähnlichkeitsmatrix erfolgreich gespeichert!\n",
      "\n",
      "🧹 Memory cleanup durchgeführt\n",
      "\n",
      "🎉 MinHash-Analyse abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n💾 SCHRITT 5: FINALE DATAFRAMES ERSTELLEN UND SPEICHERN\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Erstelle DataFrame für Ähnlichkeitsmatrix\n",
    "print(\"📊 Erstelle Ähnlichkeits-DataFrame mit pipe:IDs...\")\n",
    "df_similarity = pd.DataFrame(\n",
    "    similarity_matrix,\n",
    "    index=document_ids,\n",
    "    columns=document_ids\n",
    ")\n",
    "\n",
    "print(f\"✅ DataFrame erstellt:\")\n",
    "print(f\"   📊 Ähnlichkeitsmatrix: {df_similarity.shape}\")\n",
    "\n",
    "# Speichere die Matrix\n",
    "print(f\"\\n💾 Speichere Matrix...\")\n",
    "dataHandler.save_data(df_similarity, \"data_files.processed_data.similarity_content_based.df_minhash_text_similarity\")\n",
    "\n",
    "print(f\"✅ Ähnlichkeitsmatrix erfolgreich gespeichert!\")\n",
    "\n",
    "# Cleanup großer Variablen\n",
    "del document_minhashes, all_documents, similarity_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n🧹 Memory cleanup durchgeführt\")\n",
    "print(f\"\\n🎉 MinHash-Analyse abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cc4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_analysis-ODc0IeOL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
