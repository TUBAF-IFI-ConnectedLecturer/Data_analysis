{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fe8bbb",
   "metadata": {},
   "source": [
    "# MinHash-basierte TextÃ¤hnlichkeitsanalyse\n",
    "\n",
    "Dieses Notebook implementiert MinHash zur effizienten Berechnung der Jaccard-Ã„hnlichkeit zwischen Textdokumenten.\n",
    "\n",
    "## Konzept\n",
    "- **MinHash**: Probabilistische Datenstruktur zur SchÃ¤tzung der Jaccard-Ã„hnlichkeit\n",
    "- **Vorteil**: Skaliert sehr gut fÃ¼r groÃŸe Dokumentsammlungen\n",
    "- **Anwendung**: Direkte Textanalyse der OPAL-Materialien\n",
    "- **Output**: Ã„hnlichkeitsmatrix basierend auf Textinhalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb38282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import aller benÃ¶tigten Bibliotheken fÃ¼r MinHash\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import gc\n",
    "from datasketch import MinHash\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7132ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from DataHandler import DataHandler\n",
    "\n",
    "dataHandler = DataHandler(\"../config.yaml\")\n",
    "config_manager = dataHandler.config_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1afbde4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Lade: OPAL_ai_meta.p\n",
      "   âœ… Geladen: 4,548 Zeilen Ã— 25 Spalten\n",
      "ğŸ“Š Geladen: 4548 OPAL-Materialien\n",
      "ğŸ“ Content-Ordner: /media/sz/Data/Connected_Lecturers/Opal/raw/content\n"
     ]
    }
   ],
   "source": [
    "# Lade OPAL-Metadaten und konfiguriere Content-Ordner\n",
    "df_aimeta = dataHandler.load_data(\"data_files.raw_data.df_aimeta\")\n",
    "content_folder = dataHandler.config_manager.get('folder_structure.raw_content')\n",
    "\n",
    "print(f\"ğŸ“Š Geladen: {len(df_aimeta)} OPAL-Materialien\")\n",
    "print(f\"ğŸ“ Content-Ordner: {content_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24982d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ SCHRITT 1: DATEN VORBEREITEN\n",
      "----------------------------------------\n",
      "ğŸ”¢ Anzahl zu verarbeitender Materialien: 4548\n",
      "ğŸ“‹ pipe:ID Bereich: 1-FnRgnGtuu4 bis 9zrgYh873fJ4\n",
      "\n",
      "âš™ï¸ KONFIGURATION\n",
      "----------------------------------------\n",
      "ğŸ“„ Text-Features Konfiguration:\n",
      "   ğŸ“– Content einbeziehen: âœ…\n",
      "   ğŸ“ Titel einbeziehen: âŒ (Gewichtung: 3x)\n",
      "   ğŸ·ï¸  Keywords einbeziehen: âŒ (Gewichtung: 2x)\n",
      "   âš™ï¸ Shingle-Konfiguration: 3-gram Shingles, 128 Permutationen\n",
      "\n",
      "ğŸ”¤ SCHRITT 2: TEXT-FEATURES EXTRAHIEREN\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“„ Lade Content-Dateien und kombiniere mit Metadaten...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeite Materialien:   0%|          | 0/4548 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verarbeite Materialien: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4548/4548 [00:42<00:00, 106.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Erfolgreich verarbeitete Materialien: 4548\n",
      "âŒ Fehlgeschlagene Content-Loads: 0\n",
      "ğŸ“Š Content-VerfÃ¼gbarkeitsrate: 100.0%\n",
      "ğŸ“Š Gesamt-Verarbeitungsrate: 100.0%\n",
      "\n",
      "ğŸ“ Verwendete Text-Features:\n",
      "   1. Content (Haupttext)\n",
      "ğŸ“ˆ Shingle-Statistiken:\n",
      "   Durchschnitt: 1771.2\n",
      "   Median: 714.5\n",
      "   Min: 22\n",
      "   Max: 83437\n",
      "\n",
      "ğŸ’¾ Verwende 4548 Materialien fÃ¼r MinHash-Berechnung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Bereinigt Text fÃ¼r MinHash-Verarbeitung\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Konvertiere zu String falls noch nicht\n",
    "    text = str(text)\n",
    "    \n",
    "    # Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Entferne Interpunktion und Sonderzeichen\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Entferne mehrfache Leerzeichen\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Entferne fÃ¼hrende/nachfolgende Leerzeichen\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_shingles(text, k=3):\n",
    "    \"\"\"Erstellt k-Shingles aus Text\"\"\"\n",
    "    if not text or len(text) < k:\n",
    "        return set()\n",
    "    \n",
    "    words = text.split()\n",
    "    if len(words) < k:\n",
    "        return {' '.join(words)}\n",
    "    \n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = ' '.join(words[i:i+k])\n",
    "        shingles.add(shingle)\n",
    "    \n",
    "    return shingles\n",
    "\n",
    "def load_content_file(pipe_id, content_folder):\n",
    "    \"\"\"LÃ¤dt den Inhalt einer Content-Datei basierend auf pipe:ID\"\"\"\n",
    "    content_path = Path(content_folder)\n",
    "    \n",
    "    # Suche nach Datei mit pipe_id im Namen\n",
    "    # Format ist meist: {pipe_id}.txt oder Ã¤hnlich\n",
    "    possible_files = list(content_path.glob(f\"*{pipe_id}*\"))\n",
    "    \n",
    "    if not possible_files:\n",
    "        return \"\"\n",
    "    \n",
    "    # Nimm die erste gefundene Datei\n",
    "    file_path = possible_files[0]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Fehler beim Laden von {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"\\nğŸ“‹ SCHRITT 1: DATEN VORBEREITEN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verwende alle verfÃ¼gbaren Materialien\n",
    "valid_materials = df_aimeta.copy()\n",
    "print(f\"ğŸ”¢ Anzahl zu verarbeitender Materialien: {len(valid_materials)}\")\n",
    "\n",
    "# Erstelle Mapping von Index zu pipe:ID\n",
    "pipe_ids = valid_materials['pipe:ID'].tolist()\n",
    "print(f\"ğŸ“‹ pipe:ID Bereich: {min(pipe_ids)} bis {max(pipe_ids)}\")\n",
    "\n",
    "print(\"\\nâš™ï¸ KONFIGURATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Konfiguration fÃ¼r Text-Features (anpassbar)\n",
    "INCLUDE_CONTENT = True      # Haupttext-Content einbeziehen\n",
    "INCLUDE_TITLE = False        # Titel einbeziehen\n",
    "INCLUDE_KEYWORDS = False     # Keywords einbeziehen\n",
    "\n",
    "# Gewichtung der verschiedenen Textquellen\n",
    "TITLE_WEIGHT = 3           # Titel 3x gewichten (da sehr relevant)\n",
    "KEYWORDS_WEIGHT = 2        # Keywords 2x gewichten (wichtige SchlagwÃ¶rter)\n",
    "\n",
    "# Konfiguration fÃ¼r Shingles\n",
    "k_shingles = 3  # 3-gram Shingles\n",
    "num_perm = 128  # Anzahl Permutationen fÃ¼r MinHash\n",
    "\n",
    "print(f\"ğŸ“„ Text-Features Konfiguration:\")\n",
    "print(f\"   ğŸ“– Content einbeziehen: {'âœ…' if INCLUDE_CONTENT else 'âŒ'}\")\n",
    "print(f\"   ğŸ“ Titel einbeziehen: {'âœ…' if INCLUDE_TITLE else 'âŒ'} (Gewichtung: {TITLE_WEIGHT}x)\")\n",
    "print(f\"   ğŸ·ï¸  Keywords einbeziehen: {'âœ…' if INCLUDE_KEYWORDS else 'âŒ'} (Gewichtung: {KEYWORDS_WEIGHT}x)\")\n",
    "print(f\"   âš™ï¸ Shingle-Konfiguration: {k_shingles}-gram Shingles, {num_perm} Permutationen\")\n",
    "\n",
    "print(\"\\nğŸ”¤ SCHRITT 2: TEXT-FEATURES EXTRAHIEREN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sammle alle Text-Features\n",
    "all_documents = {}\n",
    "failed_loads = 0\n",
    "\n",
    "print(\"\\nğŸ“„ Lade Content-Dateien und kombiniere mit Metadaten...\")\n",
    "\n",
    "for idx, (df_idx, row) in enumerate(tqdm(valid_materials.iterrows(), total=len(valid_materials), desc=\"Verarbeite Materialien\")):\n",
    "    pipe_id = row['pipe:ID']\n",
    "    \n",
    "    # 1. Lade Content-Datei (optional)\n",
    "    content_text = \"\"\n",
    "    if INCLUDE_CONTENT:\n",
    "        content_text = load_content_file(pipe_id, content_folder)\n",
    "        if not content_text.strip():\n",
    "            failed_loads += 1\n",
    "    \n",
    "    # 2. Extrahiere Titel (optional)\n",
    "    title = \"\"\n",
    "    if INCLUDE_TITLE:\n",
    "        title = row.get('ai:title', '')\n",
    "        if pd.isna(title):\n",
    "            title = ''\n",
    "    \n",
    "    # 3. Extrahiere Keywords (optional) - verbesserte Behandlung fÃ¼r Arrays\n",
    "    keywords_text = \"\"\n",
    "    if INCLUDE_KEYWORDS:\n",
    "        keywords = row.get('valid_ddc_keywords', [])\n",
    "        \n",
    "        try:\n",
    "            if keywords is not None and hasattr(keywords, '__iter__') and not isinstance(keywords, str):\n",
    "                # Es ist eine Liste/Array\n",
    "                keywords_list = [str(kw).strip() for kw in keywords if kw is not None and str(kw).strip()]\n",
    "                keywords_text = ' '.join(keywords_list)\n",
    "            elif keywords is not None:\n",
    "                # Es ist ein einzelner Wert\n",
    "                keywords_text = str(keywords).strip()\n",
    "        except Exception as e:\n",
    "            # Falls etwas schiefgeht, ignoriere Keywords fÃ¼r dieses Dokument\n",
    "            keywords_text = \"\"\n",
    "    \n",
    "    # 4. Kombiniere alle aktivierten Textquellen\n",
    "    combined_text_parts = []\n",
    "    \n",
    "    if INCLUDE_CONTENT and content_text.strip():\n",
    "        combined_text_parts.append(content_text)\n",
    "    \n",
    "    if INCLUDE_TITLE and title.strip():\n",
    "        # Titel entsprechend der Gewichtung hinzufÃ¼gen\n",
    "        combined_text_parts.extend([title] * TITLE_WEIGHT)\n",
    "    \n",
    "    if INCLUDE_KEYWORDS and keywords_text.strip():\n",
    "        # Keywords entsprechend der Gewichtung hinzufÃ¼gen\n",
    "        combined_text_parts.extend([keywords_text] * KEYWORDS_WEIGHT)\n",
    "    \n",
    "    # Kombiniere zu einem Dokument\n",
    "    if combined_text_parts:\n",
    "        combined_text = ' '.join(combined_text_parts)\n",
    "        cleaned_text = clean_text(combined_text)\n",
    "        \n",
    "        # Erstelle Shingles\n",
    "        shingles = create_shingles(cleaned_text, k_shingles)\n",
    "        \n",
    "        if shingles:  # Nur wenn Shingles vorhanden\n",
    "            all_documents[pipe_id] = shingles\n",
    "    \n",
    "    # Memory cleanup alle 500 Iterationen\n",
    "    if idx % 500 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… Erfolgreich verarbeitete Materialien: {len(all_documents)}\")\n",
    "if INCLUDE_CONTENT:\n",
    "    print(f\"âŒ Fehlgeschlagene Content-Loads: {failed_loads}\")\n",
    "    print(f\"ğŸ“Š Content-VerfÃ¼gbarkeitsrate: {(len(valid_materials)-failed_loads)/len(valid_materials)*100:.1f}%\")\n",
    "print(f\"ğŸ“Š Gesamt-Verarbeitungsrate: {len(all_documents)/len(valid_materials)*100:.1f}%\")\n",
    "\n",
    "# Zeige verwendete Text-Features\n",
    "print(f\"\\nğŸ“ Verwendete Text-Features:\")\n",
    "active_features = []\n",
    "if INCLUDE_CONTENT:\n",
    "    active_features.append(f\"Content (Haupttext)\")\n",
    "if INCLUDE_TITLE:\n",
    "    active_features.append(f\"Titel (Gewichtung: {TITLE_WEIGHT}x)\")\n",
    "if INCLUDE_KEYWORDS:\n",
    "    active_features.append(f\"Keywords (Gewichtung: {KEYWORDS_WEIGHT}x)\")\n",
    "\n",
    "for i, feature in enumerate(active_features, 1):\n",
    "    print(f\"   {i}. {feature}\")\n",
    "\n",
    "if not active_features:\n",
    "    print(\"   âš ï¸ WARNUNG: Keine Text-Features aktiviert!\")\n",
    "\n",
    "# Zeige Statistiken\n",
    "if all_documents:\n",
    "    shingle_counts = [len(shingles) for shingles in all_documents.values()]\n",
    "    print(f\"ğŸ“ˆ Shingle-Statistiken:\")\n",
    "    print(f\"   Durchschnitt: {np.mean(shingle_counts):.1f}\")\n",
    "    print(f\"   Median: {np.median(shingle_counts):.1f}\")\n",
    "    print(f\"   Min: {min(shingle_counts)}\")\n",
    "    print(f\"   Max: {max(shingle_counts)}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Verwende {len(all_documents)} Materialien fÃ¼r MinHash-Berechnung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd84039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§® SCHRITT 3: MINHASH-BERECHNUNG\n",
      "----------------------------------------\n",
      "ğŸ”¢ Erstelle MinHash-Signaturen fÃ¼r 4548 Dokumente...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash Signaturen: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4548/4548 [01:12<00:00, 62.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MinHash-Signaturen erstellt fÃ¼r 4548 Dokumente\n",
      "\n",
      "ğŸ“ SCHRITT 4: Ã„HNLICHKEITSMATRIX BERECHNEN\n",
      "----------------------------------------\n",
      "ğŸ”¢ Berechne 4548 Ã— 4548 Ã„hnlichkeitsmatrix...\n",
      "ğŸ’­ Erwartete Berechnungen: 10,339,878 Paare\n",
      "ğŸ”„ Berechne paarweise Ã„hnlichkeiten...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ã„hnlichkeiten: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10339878/10339878 [00:50<00:00, 206443.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ã„hnlichkeitsmatrix berechnet: (4548, 4548)\n",
      "\n",
      "ğŸ“Š Ã„HNLICHKEITSMATRIX-STATISTIKEN:\n",
      "   ğŸ“ Matrix-GrÃ¶ÃŸe: 4548 Ã— 4548\n",
      "   ğŸ”¢ Gesamte EintrÃ¤ge: 20,684,304\n",
      "   ğŸ“ˆ Ã„hnlichkeits-Statistiken (ohne Diagonale):\n",
      "      Durchschnitt: 0.0005\n",
      "      Median: 0.0000\n",
      "      Std-Abweichung: 0.0097\n",
      "      Min: 0.0000\n",
      "      Max: 1.0000\n",
      "\n",
      "ğŸ’¾ Bereite finale DataFrames vor...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ§® SCHRITT 3: MINHASH-BERECHNUNG\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Erstelle MinHash-Signaturen fÃ¼r alle Dokumente\n",
    "document_minhashes = {}\n",
    "document_ids = list(all_documents.keys())\n",
    "\n",
    "print(f\"ğŸ”¢ Erstelle MinHash-Signaturen fÃ¼r {len(document_ids)} Dokumente...\")\n",
    "\n",
    "for idx, doc_id in enumerate(tqdm(document_ids, desc=\"MinHash Signaturen\")):\n",
    "    # Erstelle MinHash fÃ¼r dieses Dokument\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    \n",
    "    # FÃ¼ge alle Shingles zu MinHash hinzu\n",
    "    for shingle in all_documents[doc_id]:\n",
    "        m.update(shingle.encode('utf8'))\n",
    "    \n",
    "    document_minhashes[doc_id] = m\n",
    "    \n",
    "    # Memory cleanup alle 1000 Dokumente\n",
    "    if idx % 1000 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"âœ… MinHash-Signaturen erstellt fÃ¼r {len(document_minhashes)} Dokumente\")\n",
    "\n",
    "print(\"\\nğŸ“ SCHRITT 4: Ã„HNLICHKEITSMATRIX BERECHNEN\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Erstelle Ã„hnlichkeitsmatrix\n",
    "print(f\"ğŸ”¢ Berechne {len(document_ids)} Ã— {len(document_ids)} Ã„hnlichkeitsmatrix...\")\n",
    "print(f\"ğŸ’­ Erwartete Berechnungen: {len(document_ids) * (len(document_ids) - 1) // 2:,} Paare\")\n",
    "\n",
    "# Initialisiere Matrix\n",
    "similarity_matrix = np.zeros((len(document_ids), len(document_ids)))\n",
    "\n",
    "# Berechne Ã„hnlichkeiten\n",
    "total_comparisons = len(document_ids) * (len(document_ids) - 1) // 2\n",
    "processed_comparisons = 0\n",
    "\n",
    "print(\"ğŸ”„ Berechne paarweise Ã„hnlichkeiten...\")\n",
    "\n",
    "with tqdm(total=total_comparisons, desc=\"Ã„hnlichkeiten\") as pbar:\n",
    "    for i in range(len(document_ids)):\n",
    "        doc_id_1 = document_ids[i]\n",
    "        minhash_1 = document_minhashes[doc_id_1]\n",
    "        \n",
    "        # Setze Diagonale auf 1.0 (Dokument ist mit sich selbst identisch)\n",
    "        similarity_matrix[i, i] = 1.0\n",
    "        \n",
    "        for j in range(i + 1, len(document_ids)):\n",
    "            doc_id_2 = document_ids[j]\n",
    "            minhash_2 = document_minhashes[doc_id_2]\n",
    "            \n",
    "            # Berechne Jaccard-Ã„hnlichkeit\n",
    "            jaccard_sim = minhash_1.jaccard(minhash_2)\n",
    "            \n",
    "            # Symmetrische Matrix\n",
    "            similarity_matrix[i, j] = jaccard_sim\n",
    "            similarity_matrix[j, i] = jaccard_sim\n",
    "            \n",
    "            processed_comparisons += 1\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Memory cleanup alle 100 Zeilen\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "print(f\"âœ… Ã„hnlichkeitsmatrix berechnet: {similarity_matrix.shape}\")\n",
    "\n",
    "# Statistiken der Ã„hnlichkeitsmatrix\n",
    "non_diagonal_values = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "print(f\"\\nğŸ“Š Ã„HNLICHKEITSMATRIX-STATISTIKEN:\")\n",
    "print(f\"   ğŸ“ Matrix-GrÃ¶ÃŸe: {similarity_matrix.shape[0]} Ã— {similarity_matrix.shape[1]}\")\n",
    "print(f\"   ğŸ”¢ Gesamte EintrÃ¤ge: {similarity_matrix.size:,}\")\n",
    "print(f\"   ğŸ“ˆ Ã„hnlichkeits-Statistiken (ohne Diagonale):\")\n",
    "print(f\"      Durchschnitt: {np.mean(non_diagonal_values):.4f}\")\n",
    "print(f\"      Median: {np.median(non_diagonal_values):.4f}\")\n",
    "print(f\"      Std-Abweichung: {np.std(non_diagonal_values):.4f}\")\n",
    "print(f\"      Min: {np.min(non_diagonal_values):.4f}\")\n",
    "print(f\"      Max: {np.max(non_diagonal_values):.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Bereite finale DataFrames vor...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30316dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ SCHRITT 5: FINALE DATAFRAMES ERSTELLEN UND SPEICHERN\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š Erstelle Ã„hnlichkeits-DataFrame mit pipe:IDs...\n",
      "âœ… DataFrame erstellt:\n",
      "   ğŸ“Š Ã„hnlichkeitsmatrix: (4548, 4548)\n",
      "\n",
      "ğŸ’¾ Speichere Matrix...\n",
      "ğŸ’¾ Datei gespeichert: minhash_text_similarity.p\n",
      "   ğŸ“ Pfad: /media/sz/Data/Connected_Lecturers/Opal/processed/similarity/minhash_text_similarity.p\n",
      "   ğŸ“Š DataFrame: 4,548 Zeilen Ã— 4548 Spalten\n",
      "   ğŸ“ DateigrÃ¶ÃŸe: 157.9 MB\n",
      "   ğŸ• Zeitstempel: 2025-07-29 15:04:08\n",
      "   â±ï¸  Speicherdauer: 0.06 Sekunden\n",
      "âœ… Ã„hnlichkeitsmatrix erfolgreich gespeichert!\n",
      "\n",
      "ğŸ§¹ Memory cleanup durchgefÃ¼hrt\n",
      "\n",
      "ğŸ‰ MinHash-Analyse abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ’¾ SCHRITT 5: FINALE DATAFRAMES ERSTELLEN UND SPEICHERN\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Erstelle DataFrame fÃ¼r Ã„hnlichkeitsmatrix\n",
    "print(\"ğŸ“Š Erstelle Ã„hnlichkeits-DataFrame mit pipe:IDs...\")\n",
    "df_similarity = pd.DataFrame(\n",
    "    similarity_matrix,\n",
    "    index=document_ids,\n",
    "    columns=document_ids\n",
    ")\n",
    "\n",
    "print(f\"âœ… DataFrame erstellt:\")\n",
    "print(f\"   ğŸ“Š Ã„hnlichkeitsmatrix: {df_similarity.shape}\")\n",
    "\n",
    "# Speichere die Matrix\n",
    "print(f\"\\nğŸ’¾ Speichere Matrix...\")\n",
    "dataHandler.save_data(df_similarity, \"data_files.processed_data.similarity_content_based.df_minhash_text_similarity\")\n",
    "\n",
    "print(f\"âœ… Ã„hnlichkeitsmatrix erfolgreich gespeichert!\")\n",
    "\n",
    "# Cleanup groÃŸer Variablen\n",
    "del document_minhashes, all_documents, similarity_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nğŸ§¹ Memory cleanup durchgefÃ¼hrt\")\n",
    "print(f\"\\nğŸ‰ MinHash-Analyse abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cc4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_analysis-ODc0IeOL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
